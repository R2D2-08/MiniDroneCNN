# trained_parrot_minidrone
The idea of this project was to use a Convolutional neural network to classify the feed of the camera of the minidrone which would help it in traversing the track in the arena.
The CNN would be trained on every possible image that the camera would see with a few imposed constraints.
The CNN would classify each image into one of the classification labels, who's values were used by the stateflow chart to control speed and direction.
The images that would be a part of our training dataset included those that the camera would see when the drone's head was aligned along the direction of the track, if the head wasn't aligned those images would be a part of those classification labels where the drone would have to rotate about the z-axis i.e. at junctions or immediately after take-off.
Imposing these constraints the training dataset's size shrinked down to 9700 images. These images were generated using GIMP and 80% of the dataset generation was handled by @SlyPredator. A cumbersome but necessary process, once done, the images were cropped from 120*160 to 100*100 and resized down to 50*50 pixels and the CNN was trained on these images with their appropriate labels in-tact.
The CNN consists of 2 convolutional layers, 2 maxpool layers, 2 fully connected layers and a softmax one to top it off. The architecture and the number of kernels were appropriately chosen to push the parameters upto 52k.
One of the major hurdles that took countless hours to debug was that the images that the CNN was trained on were black and white but had pixel values from 0 to 255, and when simulink handles the images from the camera, after processing the images and grayscaling them, simulink outputs these as a binary map of 0s and 1s, meaning that the network that is trained on pixel values frmo 0 to 255, was passing forward images with pixel values of 0s and 1s. This caused the CNN module in the simulink model to never output the right classification label, only after further examination was it found that this was the issue.
The initial design had 8 labels in the output distribution, after much needed attention to the behaviour of the drone during the simulation, it was found that 3 of them were completely redundant. Modifying the networks, both in matlab and torch to output only 5 labels was done, after which the stateflow chart and the relevant output probabilities helped the drone in dampingly osciallate about the head of the drone until it was perfectly aligned in that particular case.
